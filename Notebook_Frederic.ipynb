{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model - training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v hyperparameters: \n",
    "vec_size = 100\n",
    "window = 7\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preprocessing train data...', end=' ', flush=True)\n",
    "labels_train, corpus_train = get_data('train')\n",
    "labels_valid, corpus_valid = get_data('dev')\n",
    "labels_test, corpus_test = get_data('test')\n",
    "print('done.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training w2v model...', end=' ', flush=True)\n",
    "    model = Word2Vec(sentences=corpus_train, vector_size=vec_size,\n",
    "                     window=window, min_count=1, workers=4, epochs=epochs)\n",
    "    model.save(\n",
    "        \"trained_models/word2vec_{}_{}_{}.model\".format(vec_size, window, epochs))\n",
    "    print('done.', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model - evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the hyperparameters for the w2v model based on the following evaluation, which yielded the parameters above\n",
    "\n",
    "this was the output from the best model: \n",
    "\n",
    "word2vec_100_7_15.model: \n",
    "\n",
    "((0.3767671252007464, 3.123816278482829e-12), SpearmanrResult(correlation=0.3488502741199042, pvalue=1.3738505633123715e-10), 9.34844192634561)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the eval metrics from word2vec word pairs: \n",
    "word_sim = model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))\n",
    "print('output from word pairs: ')\n",
    "print(word_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with word2vec embedding - preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to preprocess the data by embedding the words and averaging them, filtering out the sentences that could not be embedded, due to being empty or only having words that are not in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map text to vectors and average each sentence to one vector:\n",
    "all_vector_texts = []\n",
    "all_labels = []\n",
    "not_in_model = []\n",
    "for text, labels in zip([corpus_train, corpus_valid, corpus_test], [labels_train, labels_valid,labels_test]):\n",
    "    delete_labels = []\n",
    "    vector_text = []\n",
    "    for i,sentence in enumerate(text):\n",
    "        # assert that sentence is not empty:\n",
    "        if sentence == []:\n",
    "            delete_labels.append(i)\n",
    "            continue\n",
    "\n",
    "        feature_vec = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                feature_vec.append(w2v_model.wv[word])\n",
    "            except Exception:\n",
    "                not_in_model.append(word)\n",
    "\n",
    "        mean = np.array(feature_vec).mean(axis=0)\n",
    "        # also get rid of nan means due to only unknown words (example: sentence with 1 word not in dict)\n",
    "        if np.shape(mean) != (vec_size,):\n",
    "            delete_labels.append(i)\n",
    "            continue\n",
    "\n",
    "        vector_text.append(mean)\n",
    "    # delete labels:\n",
    "    for i in sorted(delete_labels, reverse=True):\n",
    "        del labels[i]\n",
    "\n",
    "    all_vector_texts.append(vector_text)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "train_vector_text = all_vector_texts[0]\n",
    "val_vector_text = all_vector_texts[1]\n",
    "test_vector_text = all_vector_texts[2]\n",
    "\n",
    "train_labels = all_labels[0]\n",
    "val_labels = all_labels[1]\n",
    "test_labels = all_labels[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with word2vec embedding - Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out logistic regression on top of the averaged embeddings. We did a grid search to find the parameters that worked best. (l2 penalty , C=5, class weigths = None, F1_weighted score for val and test respectively: 0.746, 0.749) Sadly we did not manage to yield better result here, than on the baseline. \n",
    "\n",
    "We also tried varying the word2vec models here, finding that vec_size=300 worked best here, although it performed worse on the word pairs task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)\n",
    "log_reg.fit(np.array(train_vector_text), np.array(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on val and test: \n",
    "def evaluate(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    micro = f1_score(y, y_pred, average='micro')\n",
    "    macro = f1_score(y, y_pred, average='macro')\n",
    "    weighted = f1_score(y, y_pred, average='weighted')\n",
    "    # samples = f1_score(y, y_pred, average='samples')\n",
    "    print(f'F1 Score: micro {micro}, macro {macro}, weighted {weighted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(log_reg, val_vector_text, val_labels)\n",
    "evaluate(log_reg, test_vector_text, test_labels)\n",
    "\n",
    "# save the model\n",
    "model_name_logreg = 'log_reg.sav'\n",
    "pickle.dump(log_reg, open(model_name_logreg, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and evaluate: \n",
    "# you can also load the model here and evaluate the results: \n",
    "loaded_model = pickle.load(open(model_name_logreg, 'rb'))\n",
    "evaluate(loaded_model, val_vector_text, val_labels)\n",
    "evaluate(loaded_model, test_vector_text, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with word2vec embedding - SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also tried out SVCs on top of the averaged embedding. They performed very promising, when training them on the validation set and evaluating on test, but we did not manage to train them on the full dataset due to computational and time limits. \n",
    "For training on validation, we also did a small hyperparameter search yielding the following parameters: \n",
    "kernel = 'rbf', C=2\n",
    "F1_weighted score on test set: 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(gamma='auto', random_state=0, C=2,verbose=True, kernel='rbf')\n",
    "svc.fit(np.array(val_vector_text), np.array(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate as above\n",
    "evaluate(svc, test_vector_text, test_labels)\n",
    "\n",
    "#save the model\n",
    "model_name_svc = 'svc_val.sav'\n",
    "pickle.dump(svc, open(model_name_svc, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the provided model and evaluate: \n",
    "# load the model and evaluate: \n",
    "# you can also load the model here and evaluate the results: \n",
    "loaded_model = pickle.load(open(model_name_svc, 'rb'))\n",
    "evaluate(loaded_model, test_vector_text, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with word2vec embedding - Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last attempt was to feed the averaged embeddings into a neural network. This seemed very promising, but the results did not look very well here. We suspect that further fine tuning of the model would have been needed here. \n",
    "The resulting F1_weighted score was 0.29. We would have wished to further investigate the issue here, which was not possible due to time constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom f1_weighted: \n",
    "def f1_weighted(label, pred):\n",
    "    label = K.cast(K.flatten(label), 'int32')\n",
    "    true = K.one_hot(label, num_classes)\n",
    "    pred_labels = K.argmax(pred, axis=-1)\n",
    "    pred = K.one_hot(pred_labels, num_classes)\n",
    "\n",
    "    ground_positives = K.sum(true, axis=0) + K.epsilon()  # = TP + FN\n",
    "    pred_positives = K.sum(pred, axis=0) + K.epsilon()  # = TP + FP\n",
    "    true_positives = K.sum(true * pred, axis=0) + K.epsilon()  # = TP\n",
    "    # all with shape (4,)\n",
    "\n",
    "    precision = true_positives / pred_positives\n",
    "    recall = true_positives / ground_positives\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "    weighted_f1 = f1 * ground_positives / K.sum(ground_positives)\n",
    "    weighted_f1 = K.sum(weighted_f1)\n",
    "\n",
    "    return weighted_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the nn\n",
    "fc_model = Sequential()\n",
    "fc_model.add(Dense(64,input_dim=vec_size, activation='relu'))\n",
    "fc_model.add(Dense(16, activation='relu'))\n",
    "fc_model.add(Dense(5, activation='softmax'))\n",
    "fc_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', metrics=[f1_weighted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the nn with callbacks and checkpoints\n",
    "model_name = 'fc_w2v_model'\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f'./logs/{model_name}_{timestr}', update_freq='batch')\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='fc_w2v_checkpoint', save_best_only=True, monitor='val_loss',\n",
    "                                                save_format='tf')\n",
    "fc_model.fit(np.array(train_vector_text).reshape(-1, 100), np.array(train_labels), epochs=50,\n",
    "             validation_data=(np.array(val_vector_text).reshape(-1, 100), np.array(val_labels)),\n",
    "             batch_size=64, callbacks=[tensorboard_callback, checkpoint])\n",
    "fc_model.save(model_name, save_format='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model and evaluate: \n",
    "loaded_model = tf.keras.models.load_model(model_name)\n",
    "# evaluate:\n",
    "val_results = model.evaluate(np.array(val_vector_text).reshape(-1, 100), np.array(val_labels))\n",
    "test_results = model.evaluate(np.array(test_vector_text).reshape(-1, 100), np.array(test_labels))\n",
    "\n",
    "print(val_results)\n",
    "print(test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
