{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "This part loads the data and prepares it for vectorization and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## import all required libraries ############\n",
    "\n",
    "# could also use the nltk one, I cannot download any package from there somehow\n",
    "from stop_words import get_stop_words\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from stop_words import get_stop_words\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from os.path import join\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import random\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables are loaded here. Among these are a library of english stop words that might need to be filtered out, a string of specific punctuation symbols that need to be filtered out, a library of contractions that can be seperated in their full words. \n",
    "The list of the 5 possible labels is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Global Variables ###########\n",
    "\n",
    "# Define the stop_words library as english\n",
    "stop_words = get_stop_words('english')\n",
    "\n",
    "# Define a string with all punctuations\n",
    "punctuations = '''!()-[]{};:'\"\\,=<>./?@#$%^&*_~'''\n",
    "\n",
    "# Define a library of contractions\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "# create a list of the possible lables\n",
    "labels = [\"background\", \"objective\", \"methods\", \"results\", \"conclusions\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different functions are created to tailor the cleaning of the text. Based on the endgoal, in this case, classifying a sentence from a scientific abstract as being from which header/part of the abstract, you might want to keep or get rid of specific words/wordparts. We have focussed specifically on scientific symbols and specific numbers that might occur more often in results or conclusion sections. Furthermore we included a function for stop-words, the WordNetLemmatizer, contractions, punctuations and accentuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Functions used for data cleaning. ###########\n",
    "\n",
    "# This function replaces specific symbols that are important for scientific context in strings so they are not removed.\n",
    "def special_symbol_replacer(sentence_list):\n",
    "    lemmatizer = {\n",
    "        '%': 'percentage',\n",
    "        '>': 'larger',\n",
    "        '<': 'smaller',\n",
    "        '+': 'plus',\n",
    "        '=': 'equals',\n",
    "        'n': 'amount',\n",
    "        '/': 'slash'}\n",
    "    new_sentence = []\n",
    "    for word in sentence_list:\n",
    "        if word in lemmatizer:\n",
    "            word = lemmatizer[word]\n",
    "        new_sentence.append(word)\n",
    "    return new_sentence\n",
    "\n",
    "# Function that replaces contractions with the two seperate words.\n",
    "def replace_contraction(list):\n",
    "    new_sentence = []\n",
    "    for word in list:\n",
    "        if word in contraction_dict:\n",
    "            new_word = contraction_dict[word]\n",
    "            new_sentence.append(new_word)\n",
    "        else:\n",
    "            new_sentence.append(word)\n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "# Function to handle numbers. Turns them into a string defining a specific category: 'integer', 'float', 'fraction'.\n",
    "# It ignores any letter/number combination words\n",
    "def handle_nums(sentence_list):\n",
    "    sentence_list = list(filter(lambda word: len(word) != 0, sentence_list))\n",
    "    output = []\n",
    "    for word in sentence_list:\n",
    "        if any(char.isdigit() for char in word):  # if there is a number in the word\n",
    "            if any(char.isalpha() for char in word):   # if there is also a letter in the word, ignore.\n",
    "                continue\n",
    "            if '.' in word:\n",
    "                output.append('float')\n",
    "            elif '/' in word:\n",
    "                output.append('fraction')\n",
    "            else:\n",
    "                output.append('integer')\n",
    "        else:\n",
    "            output.append(word)\n",
    "    return output\n",
    "\n",
    "\n",
    "# Function to handle dashes. Removes the dash and returns a word splitted by a dash in two words\n",
    "def handle_dash(sentence_list):\n",
    "    output = []\n",
    "    for word in sentence_list:\n",
    "        output += word.split('-')\n",
    "    return output\n",
    "\n",
    "\n",
    "# Function removes any single letter words from the text.\n",
    "def remove_singles(sentence_list):\n",
    "    return list(filter(lambda word: not(len(word) == 0 and word.isalpha()), sentence_list))\n",
    "\n",
    "\n",
    "# Function to perform lemmatization on the text. The lemmatizer needs to be defined elsewhere\n",
    "def lemmatizer(list):\n",
    "    # Define the lemmatizer as the WordNetLemmatizer from NLTK\n",
    "    my_lemmatizer = WordNetLemmatizer()\n",
    "    output = []\n",
    "    for word in list:\n",
    "        new_word = my_lemmatizer.lemmatize(word)\n",
    "        output.append(new_word)\n",
    "    return output\n",
    "\n",
    "\n",
    "# Function that removes all remaining punctuations\n",
    "def remove_punctuation(list):\n",
    "    output=[]\n",
    "    for word in list:\n",
    "        new_word = \"\"\n",
    "        for letter in word:\n",
    "            if letter not in string.punctuation:\n",
    "                new_word += letter\n",
    "        output.append(new_word)\n",
    "    return output\n",
    "\n",
    "\n",
    "# Function that removes all accentuated characters.\n",
    "def remove_accented_chars(list):\n",
    "    output=[]\n",
    "    for word in list:\n",
    "        output.append(unidecode(word))\n",
    "    return output\n",
    "        \n",
    "\n",
    "# Function to return all the words from each sentence back into one single string.\n",
    "def list_to_string(sentence):\n",
    "    return ' '.join(word for word in sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proprocessing function is defined. It takes as an input a text document and returns two lists of length n, containing labels and the cleaned sentences respectively in corresponding order.\n",
    "The cleaning functions are placed in specific order. Particular cleaning functions can be included or excluded. After testing the cleaning functions on the baseline classifier, it showed that no cleaning at all resulted in the best weighted F1 scores.\n",
    "A full cleaning would reduce the amount of unique words in the training text by roughly 11%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that reads whole text files, selects and splits labels and sentences, and cleans the sentences.\n",
    "def preprocess_text(text):\n",
    "    output_labels = []  # define an empty list to store the labels\n",
    "    output_sentences = []  # define an empty list to store the sentences\n",
    "\n",
    "    for line in tqdm(text):\n",
    "        lowers = line.lower()  # puts all letters in text in lowercase\n",
    "        splitted = lowers.split()  # splits the sentence in a list of words\n",
    "\n",
    "        # select only the relevant parts of the text\n",
    "        if len(splitted) > 0:  # ignores all empty lines\n",
    "            # ignores all sentences that do not start with a predifined label\n",
    "            if splitted[0] not in labels:\n",
    "                continue\n",
    "            else:\n",
    "                # split the sentence into its label and the sentence:\n",
    "                label = splitted[0]\n",
    "                labelnum = labels.index(label)\n",
    "                word_list = splitted[1:]\n",
    "\n",
    "                # Cleaning functions \n",
    "                word_list = replace_contraction(word_list)  # replaces contractions with full words\n",
    "                word_list = handle_nums(word_list)  # handeles the numbers in the text\n",
    "                word_list = special_symbol_replacer(word_list) # replaces symbol for text\n",
    "                word_list = handle_dash(word_list)  # handles words with dashes\n",
    "                word_list = remove_punctuation(word_list)   # removes punctuations\n",
    "                word_list = remove_accented_chars(word_list)   # removes accentuation\n",
    "                word_list = [word for word in word_list if not word in stop_words]\n",
    "                word_list = remove_singles(word_list)  # removes single letter words\n",
    "                word_list = [word for word in word_list if not len(word) == 0]  # removes empty strings\n",
    "#                 word_list = lemmatizer(word_list)  # Performs lemmatization\n",
    "\n",
    "                \n",
    "            # Put the obtained labels and processed text in corresponding lists.\n",
    "            output_labels.append(labelnum)\n",
    "            output_sentences.append(list_to_string(word_list))\n",
    "    return output_labels, output_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the get_data function to load the different data files and run the preprocessing function linewise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the path that stores the text files.\n",
    "path = \"./PubMed_200k_RCT\"\n",
    "\n",
    "\n",
    "# Returns [labels, sentences] pair. set type: 'test', 'dev' or 'train'\n",
    "def get_data(set_type='test'):\n",
    "    with open(join(path, f'{set_type}.txt'), \"r\") as f:\n",
    "        data = f.readlines()\n",
    "    return preprocess_text(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data of the train, test and dev file and save their labels and cleaned sentences as labels anc corpus respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model - training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v hyperparameters: \n",
    "vec_size = 100\n",
    "window = 7\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train data... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2593169/2593169 [04:02<00:00, 10697.29it/s]\n",
      "100%|██████████| 33932/33932 [00:03<00:00, 10759.31it/s]\n",
      "100%|██████████| 34493/34493 [00:03<00:00, 10944.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessing train data...', end=' ', flush=True)\n",
    "labels_train, corpus_train = get_data('train')\n",
    "labels_valid, corpus_valid = get_data('dev')\n",
    "labels_test, corpus_test = get_data('test')\n",
    "print('done.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Training w2v model...', end=' ', flush=True)\n",
    "# model = Word2Vec(sentences=corpus_train, vector_size=vec_size,\n",
    "#                  window=window, min_count=1, workers=4, epochs=epochs)\n",
    "# model.save(\n",
    "#     \"trained_models/word2vec.model\")\n",
    "# print('done.', flush=True)\n",
    "model = Word2Vec.load(\"trained_models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model - evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the hyperparameters for the w2v model based on the following evaluation, which yielded the parameters above\n",
    "\n",
    "this was the output from the best model: \n",
    "\n",
    "word2vec_100_7_15.model: \n",
    "\n",
    "((0.3767671252007464, 3.123816278482829e-12), SpearmanrResult(correlation=0.3488502741199042, pvalue=1.3738505633123715e-10), 9.34844192634561)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datapath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-a85923d9c6cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# take a look at the eval metrics from word2vec word pairs:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_word_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordsim353.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output from word pairs: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_sim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datapath' is not defined"
     ]
    }
   ],
   "source": [
    "# take a look at the eval metrics from word2vec word pairs: \n",
    "word_sim = model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))\n",
    "print('output from word pairs: ')\n",
    "print(word_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with word2vec embedding - preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to preprocess the data by embedding the words and averaging them, filtering out the sentences that could not be embedded, due to being empty or only having words that are not in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-8ce7c970b779>:21: RuntimeWarning: Mean of empty slice.\n",
      "  mean = np.array(feature_vec).mean(axis=0)\n",
      "/home/karol/.local/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# map text to vectors and average each sentence to one vector:\n",
    "all_vector_texts = []\n",
    "all_labels = []\n",
    "not_in_model = []\n",
    "for text, labels in zip([corpus_train, corpus_valid, corpus_test], [labels_train, labels_valid,labels_test]):\n",
    "    delete_labels = []\n",
    "    vector_text = []\n",
    "    for i,sentence in enumerate(text):\n",
    "        # assert that sentence is not empty:\n",
    "        if sentence == []:\n",
    "            delete_labels.append(i)\n",
    "            continue\n",
    "\n",
    "        feature_vec = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                feature_vec.append(w2v_model.wv[word])\n",
    "            except Exception:\n",
    "                not_in_model.append(word)\n",
    "\n",
    "        mean = np.array(feature_vec).mean(axis=0)\n",
    "        # also get rid of nan means due to only unknown words (example: sentence with 1 word not in dict)\n",
    "        if np.shape(mean) != (vec_size,):\n",
    "            delete_labels.append(i)\n",
    "            continue\n",
    "\n",
    "        vector_text.append(mean)\n",
    "    # delete labels:\n",
    "    for i in sorted(delete_labels, reverse=True):\n",
    "        del labels[i]\n",
    "\n",
    "    all_vector_texts.append(vector_text)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "train_vector_text = all_vector_texts[0]\n",
    "val_vector_text = all_vector_texts[1]\n",
    "test_vector_text = all_vector_texts[2]\n",
    "\n",
    "train_labels = all_labels[0]\n",
    "val_labels = all_labels[1]\n",
    "test_labels = all_labels[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with word2vec embedding - Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out logistic regression on top of the averaged embeddings. We did a grid search to find the parameters that worked best. (l2 penalty , C=5, class weigths = None, F1_weighted score for val and test respectively: 0.746, 0.749) Sadly we did not manage to yield better result here, than on the baseline. \n",
    "\n",
    "We also tried varying the word2vec models here, finding that vec_size=300 worked best here, although it performed worse on the word pairs task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-4a44f5dbec19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlog_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlog_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vector_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1340\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0m\u001b[1;32m   1343\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                                    accept_large_sparse=solver != 'liblinear')\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    796\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    617\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    620\u001b[0m                     \u001b[0;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)\n",
    "log_reg.fit(np.array(train_vector_text), np.array(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on val and test: \n",
    "def evaluate(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    micro = f1_score(y, y_pred, average='micro')\n",
    "    macro = f1_score(y, y_pred, average='macro')\n",
    "    weighted = f1_score(y, y_pred, average='weighted')\n",
    "    # samples = f1_score(y, y_pred, average='samples')\n",
    "    print(f'F1 Score: micro {micro}, macro {macro}, weighted {weighted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(log_reg, val_vector_text, val_labels)\n",
    "evaluate(log_reg, test_vector_text, test_labels)\n",
    "\n",
    "# save the model\n",
    "model_name_logreg = 'log_reg.sav'\n",
    "pickle.dump(log_reg, open(model_name_logreg, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and evaluate: \n",
    "# you can also load the model here and evaluate the results: \n",
    "loaded_model = pickle.load(open(model_name_logreg, 'rb'))\n",
    "evaluate(loaded_model, val_vector_text, val_labels)\n",
    "evaluate(loaded_model, test_vector_text, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with word2vec embedding - SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also tried out SVCs on top of the averaged embedding. They performed very promising, when training them on the validation set and evaluating on test, but we did not manage to train them on the full dataset due to computational and time limits. \n",
    "For training on validation, we also did a small hyperparameter search yielding the following parameters: \n",
    "kernel = 'rbf', C=2\n",
    "F1_weighted score on test set: 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(gamma='auto', random_state=0, C=2,verbose=True, kernel='rbf')\n",
    "svc.fit(np.array(val_vector_text), np.array(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate as above\n",
    "evaluate(svc, test_vector_text, test_labels)\n",
    "\n",
    "#save the model\n",
    "model_name_svc = 'svc_val.sav'\n",
    "pickle.dump(svc, open(model_name_svc, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the provided model and evaluate: \n",
    "# load the model and evaluate: \n",
    "# you can also load the model here and evaluate the results: \n",
    "loaded_model = pickle.load(open(model_name_svc, 'rb'))\n",
    "evaluate(loaded_model, test_vector_text, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with word2vec embedding - Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last attempt was to feed the averaged embeddings into a neural network. This seemed very promising, but the results did not look very well here. We suspect that further fine tuning of the model would have been needed here. \n",
    "The resulting F1_weighted score was 0.29. We would have wished to further investigate the issue here, which was not possible due to time constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom f1_weighted: \n",
    "def f1_weighted(label, pred):\n",
    "    label = K.cast(K.flatten(label), 'int32')\n",
    "    true = K.one_hot(label, num_classes)\n",
    "    pred_labels = K.argmax(pred, axis=-1)\n",
    "    pred = K.one_hot(pred_labels, num_classes)\n",
    "\n",
    "    ground_positives = K.sum(true, axis=0) + K.epsilon()  # = TP + FN\n",
    "    pred_positives = K.sum(pred, axis=0) + K.epsilon()  # = TP + FP\n",
    "    true_positives = K.sum(true * pred, axis=0) + K.epsilon()  # = TP\n",
    "    # all with shape (4,)\n",
    "\n",
    "    precision = true_positives / pred_positives\n",
    "    recall = true_positives / ground_positives\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "    weighted_f1 = f1 * ground_positives / K.sum(ground_positives)\n",
    "    weighted_f1 = K.sum(weighted_f1)\n",
    "\n",
    "    return weighted_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the nn\n",
    "fc_model = Sequential()\n",
    "fc_model.add(Dense(64,input_dim=vec_size, activation='relu'))\n",
    "fc_model.add(Dense(16, activation='relu'))\n",
    "fc_model.add(Dense(5, activation='softmax'))\n",
    "fc_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', metrics=[f1_weighted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the nn with callbacks and checkpoints\n",
    "model_name = 'fc_w2v_model'\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f'./logs/{model_name}_{timestr}', update_freq='batch')\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='fc_w2v_checkpoint', save_best_only=True, monitor='val_loss',\n",
    "                                                save_format='tf')\n",
    "fc_model.fit(np.array(train_vector_text).reshape(-1, 100), np.array(train_labels), epochs=50,\n",
    "             validation_data=(np.array(val_vector_text).reshape(-1, 100), np.array(val_labels)),\n",
    "             batch_size=64, callbacks=[tensorboard_callback, checkpoint])\n",
    "fc_model.save(model_name, save_format='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model and evaluate: \n",
    "loaded_model = tf.keras.models.load_model(model_name)\n",
    "# evaluate:\n",
    "val_results = model.evaluate(np.array(val_vector_text).reshape(-1, 100), np.array(val_labels))\n",
    "test_results = model.evaluate(np.array(test_vector_text).reshape(-1, 100), np.array(test_labels))\n",
    "\n",
    "print(val_results)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing sentences shorter than 10 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clead_data(labels, corpus):\n",
    "    for i, s in enumerate(corpus):\n",
    "        if len(s) < 10:\n",
    "            corpus.pop(i)\n",
    "            labels.pop(i)\n",
    "    assert(len(labels) == len(corpus))\n",
    "    return labels, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "The model takes a strings as the input and outputs integer coressponding to the predicted classes. It consists of an encoder as the first layer, which converts strings into integer lists, word2vec embedding as the second one, one LSTM layer, one densely connected with ReLu activation and softmax classifier at the end. The embedding layes is initialized using the W matrix from a pretrained gensim model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_model():\n",
    "    # Decoders transforms sentences into lists of integers.\n",
    "    encoder = TextVectorization(max_tokens=model.wv.vectors.shape[0])\n",
    "    encoder.adapt(corpus_valid)\n",
    "    \n",
    "    LSTM_model = Sequential()\n",
    "    LSTM_model.add(encoder)\n",
    "    LSTM_model.add(Embedding(input_dim=model.wv.vectors.shape[0], output_dim=model.wv.vectors.shape[1],\n",
    "                        embeddings_initializer=Constant(model.wv.vectors), trainable=False, mask_zero=True))\n",
    "    LSTM_model.add(LSTM(units=model.wv.vectors.shape[1]))\n",
    "    LSTM_model.add(Dense(units=64, activation='relu'))\n",
    "    LSTM_model.add(Dense(units=5, activation='softmax'))\n",
    "    \n",
    "    LSTM_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[])\n",
    "    \n",
    "    return LSTM_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(model):\n",
    "    history = model.fit(x=corpus_train, y=labels_train, validation_data=(corpus_valid, labels_valid),\n",
    "                        epochs=5)\n",
    "\n",
    "    model.save('LSTM_model.h5')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.2 GiB for an array with shape (2207185,) and data type <U1480",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-fbb737c91a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabels_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclead_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mLSTM_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_LSTM_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-b7b8606f92eb>\u001b[0m in \u001b[0;36mtrain_LSTM\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     history = model.fit(x=corpus_train, y=labels_train, validation_data=(corpus_valid, labels_valid),\n\u001b[0m\u001b[1;32m      3\u001b[0m                         epochs=5)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LSTM_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1047\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[1;32m   1050\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m                **kwargs):\n\u001b[1;32m    641\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 12.2 GiB for an array with shape (2207185,) and data type <U1480"
     ]
    }
   ],
   "source": [
    "labels_train, corpus_train = clead_data(labels_train, corpus_train)\n",
    "labels_valid, corpus_valid = clead_data(labels_valid, corpus_valid)\n",
    "LSTM_model = create_LSTM_model()\n",
    "history = train_LSTM(LSTM_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "The metrics and loss values indicates that the model is converging; however, after several thousands of samples it throws the following error: ###ERROR### LOG Due to a very long training time we didn't manage to find the source of these error. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
